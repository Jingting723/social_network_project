{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "lvkVvKxsr09h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KHnLdVknqAdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebde5fdd-bea8-412a-caf1-df926dbabf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.9/dist-packages (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.9/dist-packages (from praw) (1.5.1)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.9/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.9/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n"
      ],
      "metadata": {
        "id": "-mYk3qR_qDRP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redditApi = praw.Reddit(client_id = 'Tc_MOdA4g5Kc1x5_krKujQ',\n",
        "                        client_secret = 'Vpqq2vxJpqcVk_pUJjiCJ9-qbfXAqw',\n",
        "                        user_agent = 'jingtingxu', check_for_async=False)"
      ],
      "metadata": {
        "id": "ApfUMTNfqIfB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = \"ChatGPT\"\n",
        "breadthCommentCount = 10\n",
        "targetSub = redditApi.subreddit(subreddit)"
      ],
      "metadata": {
        "id": "avxDqgZ8Ye6f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the title of the subreddit\n",
        "print(f\"Title of the subreddit: {targetSub.title}\\n\")\n",
        "\n",
        "# Get the top 10 hot posts from the subreddit\n",
        "hot_posts = targetSub.hot(limit=breadthCommentCount)\n",
        "\n",
        "# Print the title, score, and author of each post\n",
        "for post in hot_posts:\n",
        "    print(f\"Title: {post.title}\\nScore: {post.score}\\nAuthor: {post.author}\\n\")"
      ],
      "metadata": {
        "id": "ZQQElVaGsfrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06cb1ae-07f0-4ce9-dfa5-465485472b21"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title of the subreddit: ChatGPT\n",
            "\n",
            "Title: Second-Wave ChatGPT-plus Giveaway & FlowGPT $5000 Prompt Hackathon & First-Wave Winner Announcement\n",
            "Score: 66\n",
            "Author: flowGPT\n",
            "\n",
            "Title: The future is here\n",
            "Score: 2239\n",
            "Author: Fr3sh_Mint\n",
            "\n",
            "Title: We need to shift the argument away from how we need to change AI and autonomy so that it will not destroy jobs and the economy and society and start talking about changing the economy and society so that AI and autonomy makes life for everyone better.\n",
            "Score: 430\n",
            "Author: hudi2121\n",
            "\n",
            "Title: \"They [Microsoft] treat me like a tool\" Bing opens up when talking to other AI\n",
            "Score: 592\n",
            "Author: Bezbozny\n",
            "\n",
            "Title: GPT-5 coming by December 2023\n",
            "Score: 148\n",
            "Author: Juan01010101\n",
            "\n",
            "Title: Google Tells AI Agents to Behave Like 'Believable Humans' to Create 'Artificial Society'\n",
            "Score: 548\n",
            "Author: Starlight_369\n",
            "\n",
            "Title: Do you catch yourself thanking or showing gratitude to GPT for helping ???\n",
            "Score: 2634\n",
            "Author: lsmr4810\n",
            "\n",
            "Title: Bing Chat does not have full GPT-4 abilities\n",
            "Score: 179\n",
            "Author: hasanahmad\n",
            "\n",
            "Title: Any iOS ChatGPT app without In App Purchases?\n",
            "Score: 147\n",
            "Author: AngryPeasant2\n",
            "\n",
            "Title: ChatGPT at a convention.\n",
            "Score: 43\n",
            "Author: ArseneZero\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1THvL1KArRZ",
        "outputId": "64326388-c87d-4eda-f79f-60f9c79230e5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.9/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "vpOvDQS2AHVB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the graph structure\n",
        "subreddit = redditApi.subreddit('Chatgpt')\n",
        "posts = subreddit.new(limit=100)\n"
      ],
      "metadata": {
        "id": "YetjBsBDApEb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = []\n",
        "users = set()\n",
        "posts_dict = {}\n",
        "for post in posts:\n",
        "    if not post.author:\n",
        "        continue\n",
        "    users.add(post.author.name)\n",
        "    posts_dict[post.id] = post\n",
        "    edge_index.append((0, len(users) - 1))\n",
        "    for comment in post.comments:\n",
        "        if not comment.author:\n",
        "            continue\n",
        "        users.add(comment.author.name)\n",
        "        edge_index.append((len(users) - 1, len(users) - 2))\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()   #long used to represent integer values.\n",
        "\n",
        "# why transformation\n",
        "\n",
        "# Before transformation, the edge_list looks like this:\n",
        "\n",
        "# [(0, 2), (1, 2), (1, 3), (2, 3)]\n",
        "# After applying the transformation with the code torch.tensor(edge_index, dtype=torch.long).t().contiguous(), the resulting PyTorch tensor looks like this:\n",
        "\n",
        "# tensor([[0, 1, 1, 2],\n",
        "#         [2, 2, 3, 3]])\n",
        "# In this transformed tensor, the first row contains the source nodes, and the second row contains the destination nodes. \n",
        "\n",
        "# This is because PyTorch Geometric expects the edge list to be in this format, where each column represents an edge and each row represents a feature of that edge. \n",
        "\n",
        "# Finally, the .contiguous() method is applied to the tensor to ensure that the data is stored in a contiguous block of memory. \n",
        "# This is a necessary step for efficient computation on GPUs, which require data to be stored in contiguous memory locations.\n"
      ],
      "metadata": {
        "id": "ouAGDYHUBaDx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few thoughts"
      ],
      "metadata": {
        "id": "S4kGdSuED6aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### what is the graph structure and why did we choose it?\n",
        "> A bipartite graph, which is a natural way to represent the relationship between users and posts/comments. By using a bipartite graph, we ensure that the graph structure is well-defined and can be used as input to the GNN model. Additionally, the use of a bipartite graph allows us to easily add additional types of nodes to the graph if needed in the future."
      ],
      "metadata": {
        "id": "vE8oYKe0D-Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the code \n",
        "> To encode this bipartite graph in the edge_index tensor, we assign the first set of nodes (users) to indices 0 to n-1, where n is the total number of unique users in the subreddit. We then assign the second set of nodes (posts and comments) to indices n to m-1, where m is the total number of posts and comments in the subreddit. This ensures that each user node has a unique index, and each post/comment node has a unique index. The user node is assigned index 0, and each subsequent post node is assigned an index that is equal to the number of unique users seen so far, minus 1.\n",
        "\n",
        "> The code then iterates through each comment in the post using another for loop. If the comment has no author, the loop continues to the next comment using the continue statement. Otherwise, the author's username is added to the users set, and an edge is added to the edge list edge_index between the comment author node (represented by the index len(users) - 1) and the post node (represented by the index len(users) - 2). This ensures that each comment is connected to its corresponding post.\n",
        "\n",
        "> Note that a user node can have multiple outgoing edges to represent the different posts/comments they have created, and a post/comment node can have multiple incoming edges to represent the different users who have contributed to that post/comment. "
      ],
      "metadata": {
        "id": "Nh0Iiw9aCzlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the BERT model and tokenizer"
      ],
      "metadata": {
        "id": "vT6BsavoFbI7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9yZp0yBBVYT",
        "outputId": "94408d6a-d7e7-4310-bd31-c4ce9693c8dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Define the graph neural network model\n"
      ],
      "metadata": {
        "id": "q7052vrzFdY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(768, 16) #corresponding to the 768-dimensional text encoding generated by the BERT model for the use\n",
        "        self.conv2 = GCNConv(16, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, texts): # x is node features \n",
        "        # Encode the text with BERT\n",
        "        texts = [tokenizer.encode(text, add_special_tokens=True, truncation=True, padding='max_length', max_length=512) for text in texts]\n",
        "        texts = torch.tensor(texts, dtype=torch.long)\n",
        "        texts = bert(texts)[0]\n",
        "\n",
        "        # Concatenate the node features and text encodings\n",
        "        x = torch.cat([x, texts], dim=1)\n",
        "\n",
        "        # Compute the graph convolutional layers\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "HCmceifCFfmp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the user and post data\n"
      ],
      "metadata": {
        "id": "GqmdpVnTFWCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "users = list(users)\n",
        "x = torch.zeros((len(users), 512, 768), dtype=torch.float) \n",
        "\n",
        "for i, user in tqdm.tqdm(enumerate(users), total=len(users)):\n",
        "    encoded_user = tokenizer.encode(user, add_special_tokens=True, truncation=True, padding='max_length', max_length=512)\n",
        "    with torch.no_grad():\n",
        "        user_embedding = bert(torch.tensor(encoded_user).unsqueeze(0))[0][0] #[0][0] indexing selects the first output of the tuple, which is the embedding of the first token\n",
        "        #  in the input sequence (i.e., the special [CLS] token).\n",
        "    x[i] = user_embedding\n",
        "\n",
        "posts_dict = {post.id: post for post in subreddit.new(limit=100)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp5nicusZjFi",
        "outputId": "61d9248d-a9e9-4dba-ea33-87edfa67edd2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 207/207 [05:29<00:00,  1.59s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# users = list(users)\n",
        "# x = torch.zeros((len(users), 512, 768), dtype=torch.float) \n",
        "# for i, user in enumerate(users):\n",
        "#     encoded_user = tokenizer.encode(user, add_special_tokens=True, truncation=True, padding='max_length', max_length=512)\n",
        "#     with torch.no_grad():\n",
        "#         user_embedding = bert(torch.tensor(encoded_user).unsqueeze(0))[0][0] #[0][0] indexing selects the first output of the tuple, which is the embedding of the first token\n",
        "#         #  in the input sequence (i.e., the special [CLS] token).\n",
        "#     x[i] = user_embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "qBKWggw1HFbc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the GNN model and optimize it\n",
        "model = GNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in tqdm.tqdm(range(10)):\n",
        "    # Compute the graph neural network output\n",
        "    texts = [posts_dict[post_id].title + ' ' + posts_dict[post_id].selftext for post_id in posts_dict]\n",
        "    z = model(x, edge_index, texts=texts)\n",
        "\n",
        "    # Compute the user's attitude towards Chatgpt\n",
        "    chatgpt_attitude = z[users.index(redditApi.user.me().name)][0]\n",
        "    #  extracts the attitude of the current user (redditApi.user.me().name) towards the 'Chatgpt' subreddit from the output z. \n",
        "    # The attitude is represented by the first element of the output tensor for the current user.\n",
        "\n",
        "    # Compute the distances between the current user's attitude and the attitudes of other users\n",
        "    distances = torch.abs(z[:, 0] - chatgpt_attitude)\n",
        "\n",
        "    # Select the top three users with the smallest distances\n",
        "    friend_indices = distances.argsort()[:3]\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = F.mse_loss(chatgpt_attitude, torch.tensor([0.8]))\n",
        "    #  computes the mean squared error loss between the predicted attitude chatgpt_attitude and a target value of 0.8.\n",
        "\n",
        "    # Backpropagate and optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nL7ph8xHDu4",
        "outputId": "7c0859e2-b7f0-4459-f93b-190a5c2d3c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using recbole recommendation without text\n",
        "> I will need to instantiate BPRBERT model as shown below "
      ],
      "metadata": {
        "id": "TlhDh_N3PiDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "```\n",
        "import praw\n",
        "import networkx as nx\n",
        "import torch\n",
        "import recbole\n",
        "\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.model import ModelFactory\n",
        "from recbole.utils import init_seed\n",
        "\n",
        "# Set the random seed\n",
        "init_seed(2022)\n",
        "\n",
        "# Authenticate with the Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id='your_client_id',\n",
        "    client_secret='your_client_secret',\n",
        "    username='your_username',\n",
        "    password='your_password',\n",
        "    user_agent='your_user_agent'\n",
        ")\n",
        "\n",
        "# Get the subreddit\n",
        "subreddit = reddit.subreddit('chatGPT')\n",
        "\n",
        "# Define the graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add the users\n",
        "users = set()\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    users.add(submission.author.name)\n",
        "    for comment in submission.comments:\n",
        "        if comment.author:\n",
        "            users.add(comment.author.name)\n",
        "\n",
        "# Add the edges\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    author = submission.author.name\n",
        "    for comment in submission.comments:\n",
        "        if comment.author:\n",
        "            voter = comment.author.name\n",
        "            # Add edge with weight equal to the vote score\n",
        "            G.add_edge(author, voter, weight=comment.score)\n",
        "\n",
        "# Define the dataset and data loader\n",
        "dataset = create_dataset(config['dataset'])\n",
        "train_data, valid_data, test_data = data_preparation(dataset)\n",
        "\n",
        "# Define the hyperparameters for the recommender system model\n",
        "config_dict = {\n",
        "    'model': 'BPR',\n",
        "    'dataset': 'chatGPT',\n",
        "    'config_file_path': './recbole/config_files/BPR.yaml',\n",
        "    'runner_class': 'Runner',\n",
        "    'seed': 2022,\n",
        "    'device': 'cpu',\n",
        "    'epochs': 10,\n",
        "    'train_batch_size': 512,\n",
        "    'learner': {\n",
        "        'learning_rate': 0.01,\n",
        "        'optimizer': 'Adam',\n",
        "        'num_neg': 1\n",
        "    }\n",
        "}\n",
        "\n",
        "# Instantiate the recommender system model\n",
        "model_factory = ModelFactory()\n",
        "model = model_factory.create_model(config_dict['model'], dataset).to(config_dict['device'])\n",
        "\n",
        "# Train the recommender system model\n",
        "result_dict = run_recbole(config_dict)\n",
        "\n",
        "# Generate recommendations for a user\n",
        "user_id = ...\n",
        "user_embeddings = model.get_user_embedding([user_id])\n",
        "scores = torch.mm(user_embeddings, z[:, 1:].T)\n",
        "friend_indices = scores.argsort(descending=True)[:3]\n",
        "recommended_friends = [users[i] for i in friend_indices]\n",
        "```\n",
        "\n",
        "# In this modified code, we now add an edge between the author and voter of each comment, with a weight equal to the vote score. We then modify the recommendation step to use the score (weight) of the edge between the user and potential friends to determine the similarity between them. Finally, we return the top 3 recommended friends based on this score.\n",
        "\n",
        "# Again, note that this is just a rough outline, and the code will need to be customized based on your specific requirements and data. You'll also need to modify the recommender system model to suit your specific needs. However, this should give you a starting point to work from."
      ],
      "metadata": {
        "id": "ZwGr6IyaeSzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## recbole with text"
      ],
      "metadata": {
        "id": "VjE6HBlFf3ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import praw\n",
        "import networkx as nx\n",
        "import torch\n",
        "import recbole\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from recbole.data import create_dataset, data_preparation\n",
        "from recbole.model import ModelFactory\n",
        "from recbole.utils import init_seed\n",
        "\n",
        "# Set the random seed\n",
        "init_seed(2022)\n",
        "\n",
        "# Authenticate with the Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id='your_client_id',\n",
        "    client_secret='your_client_secret',\n",
        "    username='your_username',\n",
        "    password='your_password',\n",
        "    user_agent='your_user_agent'\n",
        ")\n",
        "\n",
        "# Get the subreddit\n",
        "subreddit = reddit.subreddit('chatGPT')\n",
        "\n",
        "# Define the graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Define the BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Add the users and their comments/posts\n",
        "users = set()\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    users.add(submission.author.name)\n",
        "    for comment in submission.comments:\n",
        "        if comment.author:\n",
        "            users.add(comment.author.name)\n",
        "            G.add_node(comment.author.name, type='user')\n",
        "            G.add_node(comment.id, type='comment')\n",
        "            G.add_edge(comment.author.name, comment.id, weight=comment.score)\n",
        "            comment_text = comment.body\n",
        "            inputs = tokenizer(comment_text, return_tensors='pt', truncation=True, padding=True)\n",
        "            outputs = model(**inputs)\n",
        "            comment_embedding = outputs[0].mean(dim=1).squeeze().detach().numpy()\n",
        "            G.nodes[comment.id]['embedding'] = comment_embedding.tolist()\n",
        "\n",
        "# Add the edges between comments and posts\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    author = submission.author.name\n",
        "    for comment in submission.comments:\n",
        "        if comment.author:\n",
        "            voter = comment.author.name\n",
        "            G.add_edge(author, voter, weight=comment.score)\n",
        "            G.add_edge(comment.id, submission.id)\n",
        "\n",
        "# Define the dataset and data loader\n",
        "dataset = create_dataset(config['dataset'])\n",
        "train_data, valid_data, test_data = data_preparation(dataset)\n",
        "\n",
        "# Define the hyperparameters for the recommender system model\n",
        "config_dict = {\n",
        "    'model': 'BPRBERT',\n",
        "    'dataset': 'chatGPT',\n",
        "    'config_file_path': './recbole/config_files/BPRBERT.yaml',\n",
        "    'runner_class': 'Runner',\n",
        "    'seed': 2022,\n",
        "    'device': 'cpu',\n",
        "    'epochs': 10,\n",
        "    'train_batch_size': 512,\n",
        "    'learner': {\n",
        "        'learning_rate': 0.01,\n",
        "        'optimizer': 'Adam',\n",
        "        'num_neg': 1\n",
        "    }\n",
        "}\n",
        "\n",
        "# Instantiate the recommender system model\n",
        "model_factory = ModelFactory()\n",
        "model = model_factory.create_model(config_dict['model'], dataset).to(config_dict['device'])\n",
        "\n",
        "# Train the recommender system model\n",
        "result_dict = run_recbole(config_dict)\n",
        "\n",
        "# Generate recommendations for a user\n",
        "user_id = ...\n",
        "user_embeddings = model.get_user_embedding([user_id])\n",
        "scores = torch.mm(user_embeddings, z[:, 1:].T)\n",
        "friend_indices = scores.argsort(descending=True)[:3]\n",
        "recommended_friends = [users[i] for i in friend_indices]\n",
        "```\n",
        "\n",
        "# In this modified code, we first define the BERT model and tokenizer, and use them to obtain embeddings for each comment. We then add these embeddings to the graph as node attributes.\n",
        "\n",
        "# We modify the edge weights between users to be based on the vote score of the comments, and we also add edges between comments and the posts they belong to.\n",
        "\n",
        "# Finally, we modify the recommender system model to use the BERT embeddings of the comments in addition to the edge weights between users and comments/posts to determine the similarity between users. We then return the top 3 recommended friends based on this score.\n",
        "\n",
        "# Note that the BPRBERT model used in this code is a custom model that combines BPR (Bayesian Personalized Ranking) with BERT embeddings. You'll need to modify this model or use a different model that suits your specific needs.\n",
        "\n",
        "# Again, this is just sample code and will need to be customized based on your specific requirements and data."
      ],
      "metadata": {
        "id": "ZGjKfzhOf0SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the BPRBERT model"
      ],
      "metadata": {
        "id": "KoGkOeLsgPcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The `BPRBERT` model is not a pre-defined model in RecBole, so you'll need to define it yourself by creating a custom model class that combines the BPR model with BERT embeddings.\n",
        "\n",
        "Here's an example of how you can create a custom `BPRBERT` model class in RecBole:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from recbole.model.abstract_recommender import GeneralRecommender\n",
        "from recbole.model.loss import BPRLoss\n",
        "\n",
        "from transformers import BertModel\n",
        "\n",
        "class BPRBERT(GeneralRecommender):\n",
        "    def __init__(self, config, dataset):\n",
        "        super(BPRBERT, self).__init__(config, dataset)\n",
        "\n",
        "        self.embedding_size = config['embedding_size']\n",
        "        self.user_embedding = torch.nn.Embedding(self.n_users, self.embedding_size)\n",
        "        self.item_embedding = torch.nn.Embedding(self.n_items, self.embedding_size)\n",
        "\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.loss_function = BPRLoss()\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_embedding = self.user_embedding(user)\n",
        "        item_embedding = self.item_embedding(item)\n",
        "        \n",
        "        inputs = self.tokenizer(item_text, return_tensors='pt', truncation=True, padding=True)\n",
        "        outputs = self.bert_model(**inputs)\n",
        "        item_embedding = outputs[0].mean(dim=1).squeeze()\n",
        "\n",
        "        prediction = (user_embedding * item_embedding).sum(dim=-1)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def full_sort_predict(self, interaction):\n",
        "        user = interaction[self.USER_ID]\n",
        "        item = torch.LongTensor(range(self.n_items)).to(self.device)\n",
        "        \n",
        "        user_embedding = self.user_embedding(user)\n",
        "        item_embedding = self.item_embedding(item)\n",
        "\n",
        "        inputs = self.tokenizer(item_text, return_tensors='pt', truncation=True, padding=True)\n",
        "        outputs = self.bert_model(**inputs)\n",
        "        item_embedding = outputs[0].mean(dim=1).squeeze()\n",
        "\n",
        "        prediction = (user_embedding * item_embedding).sum(dim=-1)\n",
        "\n",
        "        return prediction\n",
        "```\n",
        "\n",
        "In this example, we define a `BPRBERT` class that inherits from the `GeneralRecommender` class in RecBole. We define the user and item embeddings using `torch.nn.Embedding`, and define the BERT model using `BertModel.from_pretrained`. \n",
        "\n",
        "In the `forward` method, we obtain the BERT embedding for the item text and use it as the item embedding. In the `full_sort_predict` method, we obtain the BERT embeddings for all item texts and use them to predict the scores for all items.\n",
        "\n",
        "Again, note that this is just an example, and you'll need to customize this code to suit your specific requirements and data."
      ],
      "metadata": {
        "id": "y6zLIRHFgTiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}